# cache

[code:agent/utils/cache.py]

## Content

```python
"""Caching implementations for LLM responses.

This module provides a robust caching system for storing and retrieving LLM (Language Learning Model)
responses to optimize performance and reduce redundant API calls. It includes functionality for:

    - File-based caching with TTL (Time-To-Live) support
    - Both synchronous and asynchronous function caching
    - Customizable serialization (pickle or JSON)
    - Automatic key generation from function arguments
    - Safe handling of non-seriali...
```